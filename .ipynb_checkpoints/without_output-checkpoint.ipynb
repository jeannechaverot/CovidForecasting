{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "from funcs import *\n",
    "import seaborn as seabornInstance \n",
    "#from sklearn.model_selection import train_test_split \n",
    "#from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.concat([X, y_recovered, y_deaths, y_recovered_smoothed, y_deaths_smoothed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of infected for past two weeks\n",
    "X = pd.read_csv('data.csv').iloc[:,1:-3].values\n",
    "\n",
    "#Number of recovered\n",
    "y_recovered = pd.read_csv('data.csv').iloc[:,-3].values\n",
    "\n",
    "#Number of recovered with transformation to smooth data\n",
    "y_rec_smoothed = pd.read_csv('data.csv').iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All different smoothing that I have tried:\n",
    "- simple exponential smoothing: smaller error:0.19\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_beta():\n",
    "    \"\"\"Returns optimal alpha such that MAPE error is minimized,along with the MAPE index error in question, and its value\"\"\"\n",
    "    X_new = np.zeros(X.shape)\n",
    "    betas = np.linspace(0,1,100)\n",
    "    mapes = []\n",
    "    pct_80 = int(np.ceil(80*len(X)/100))\n",
    "\n",
    "\n",
    "    for beta in betas:\n",
    "        for j in range(X.shape[1]):\n",
    "            #X_new[:,j]= SimpleExpSmoothing(X[:,j]).fit(smoothing_level=alpha,optimized=False).fittedvalues\n",
    "            X_new[:,j]= ExponentialSmoothing(X[:,j], damped=False).fit(smoothing_level=0.9595959595959597, smoothing_slope=beta).fittedvalues\n",
    "\n",
    "        X_train, X_test = X_new[:pct_80], X_new[pct_80:]\n",
    "        y_train, y_test =y_rec_smoothed[:pct_80], y_rec_smoothed[pct_80:]\n",
    "\n",
    "\n",
    "        index = find_best_k(X_train, y_train, X_test, y_test, 'mape')\n",
    "        P, q, G, h = generate_params(X_train, y_train, index)\n",
    "        gamma = cvxopt_solve_qp(P, q, G, h)\n",
    "        y_pred = X_test@gamma\n",
    "\n",
    "        mapes.append(mape(y_test, y_pred))\n",
    "\n",
    "    return beta[np.argmin(mapes)],np.argmin(mapes), min(mapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({r'$\\alpha=0.2$': exponential_smoothing(X[:,0], 0.2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = find_best_beta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f find best alpha with beta=0.2\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f finding best beta with alpha = 0.95959595 Holt damped\n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f finding the best beta with alpha = 0.9596 ExponentialSmoothing not damped\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = np.zeros(X.shape)\n",
    "find_best_alpha()\n",
    "\n",
    "for j in range(X.shape[1]):\n",
    "    new_X[:,j] = exponential_smoothing(X[:,j], 0.1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_smoothing(x, rho, K):\n",
    "    const = (1-rho)/(1-rho**(K+1))\n",
    "    new_x = []\n",
    "    \n",
    "    # range of x\n",
    "    r_x = np.arange(K, len(x)-K)\n",
    "\n",
    "    # range of k\n",
    "    r_k = np.arange(0,K)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        if i not in r_x:\n",
    "            new_x.append(x[i])\n",
    "        else:\n",
    "            ls = []\n",
    "            for k in r_k:\n",
    "                ls.append(int(const*rho**k*x[i-k]))\n",
    "            new_x.append(np.sum(ls))\n",
    "                \n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_alpha():\n",
    "    \"\"\"Returns optimal alpha such that MAPE error is minimized,along with the MAPE index error in question, and its value\"\"\"\n",
    "    X_new = np.zeros(X.shape)\n",
    "    rhos = np.linspace(0,1,10)\n",
    "    pct_80 = int(np.ceil(80*len(X)/100))\n",
    "    Ks = np.linspace(3,10)\n",
    "    mapes = np.zeros((len(Ks), len(rhos)))\n",
    "\n",
    "\n",
    "    for i, K in enumerate(Ks):\n",
    "        for j, rho in enumerate(rhos):\n",
    "            for j in range(X.shape[1]):\n",
    "                X_new[:,j]= exponential_smoothing(X[:,j], rho, 5)\n",
    "\n",
    "\n",
    "            X_train, X_test = X_new[:pct_80], X_new[pct_80:]\n",
    "            y_train, y_test =y_rec_smoothed[:pct_80], y_rec_smoothed[pct_80:]\n",
    "\n",
    "\n",
    "            index = find_best_k(X_train, y_train, X_test, y_test, 'mape')\n",
    "            P, q, G, h = generate_params(X_train, y_train, index)\n",
    "            gamma = cvxopt_solve_qp(P, q, G, h)\n",
    "            y_pred = X_test@gamma\n",
    "\n",
    "            mapes[i, j] = mape(y_test, y_pred)\n",
    "    # return optimal K, optimal rho,\n",
    "    return rhos[np.argmin(mapes)[0]], rhos[np.argmin(mapes)[1]], np.argmin(mapes), min(mapes), mapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exponential_smoothing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3a3bb22e4e0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_best_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-c45db37cf55d>\u001b[0m in \u001b[0;36mfind_best_alpha\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mX_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mexponential_smoothing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exponential_smoothing' is not defined"
     ]
    }
   ],
   "source": [
    "f = find_best_alpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.zeros(X.shape)\n",
    "for j in range(X.shape[1]):\n",
    "    X_new[:,j] = exponential_smoothing(X[:,j], 0.12121212121212122,5)\n",
    "\n",
    "\n",
    "X_train, X_test = X_new[:pct_80], X_new[pct_80:]\n",
    "y_train, y_test =y_rec_smoothed[:pct_80], y_rec_smoothed[pct_80:]\n",
    "\n",
    "index = find_best_k(X_train, y_train, X_test, y_test, 'mape')\n",
    "P, q, G, h = generate_params(X_train, y_train, index)\n",
    "gamma = cvxopt_solve_qp(P, q, G, h)\n",
    "y_pred = X_test@gamma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- Create matrix M\n",
    "- Create matrix X (DONE)\n",
    "- Compute X^TX\n",
    "- Compute M^TM\n",
    "- Verify M^TM value, if it coincides with the one G.O. wrote in report\n",
    "- install library, define instances, run optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_90 = int(np.ceil(90*len(X)/100))\n",
    "pct_80 = int(np.ceil(80*len(X)/100))\n",
    "pct_70 = int(np.ceil(70*len(X)/100))\n",
    "\n",
    "X_train, X_test = X[:pct_80], X[pct_80:]\n",
    "y_train, y_test =y_rec_smoothed[:pct_80], y_rec_smoothed[pct_80:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "\n",
    "def create_M(N):\n",
    "    M = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i==0:\n",
    "                if j == 0:\n",
    "                    M[i,j]=1\n",
    "                else:\n",
    "                    M[i,j]=0\n",
    "            elif (i==j):\n",
    "                M[i,j]=1\n",
    "            elif (j == (i-1)):\n",
    "                M[i,j] = -1\n",
    "            else:\n",
    "                M[i,j]=0\n",
    "    return M\n",
    "\n",
    "def generate_G(index):\n",
    "    \"\"\"index: represents k^*, gamma_{k^*} is such that gamma_0 <= gamma_1 <= ...<= gamma_{k^*} >= ... >= gamma_N\n",
    "    This function generates a matrix G such that either gamma_index or gamma_{index+1} is the maximum\n",
    "    \"\"\" \n",
    "    #this constraint verifies the gaussian-like distribution of the gamma\n",
    "    G = np.zeros((N,N))\n",
    "    for i in range(0, index):\n",
    "        for j in range(N):\n",
    "            if (i==j):\n",
    "                G[i,j] = 1\n",
    "            elif (j == i+1):\n",
    "                G[i,j] = -1\n",
    "                \n",
    "    for i in range(index, N):\n",
    "        for j in range(N):\n",
    "            if (i==j):\n",
    "                G[i,j] = -1\n",
    "            elif (j == i+1):\n",
    "                G[i,j] = 1\n",
    "    \n",
    "    # we do not put any condition on idx_th element, and use this line to verify that all gammas are superior or\n",
    "    # equal to zero\n",
    "    #G[index,:] = 0\n",
    "    #G[index, 0] = -1\n",
    "\n",
    "    \n",
    "    #this constraint verifies that -gamma_i <= 0 <=> gamma_i >= 0 forall i\n",
    "   # for i in range(N, 2*N):\n",
    "    #    for j in range(N):\n",
    "     #       if (i==N+j):\n",
    "      #          G[i,j]=-1\n",
    "    return G\n",
    "\n",
    "def generate_params(X_train, y_train,k,lambda_=1.0):\n",
    "    M = create_M(N)\n",
    "    M_tilde = M.T @ M\n",
    "    X_tilde = X_train.T @ X_train\n",
    "    P = X_tilde + lambda_*(M_tilde)\n",
    "    q = -X_train.T@y_train\n",
    "    G = generate_G(k)\n",
    "    h = np.zeros((N,1))\n",
    "    for i in range(len(h)):\n",
    "        h[i] = -0.0000001\n",
    "    return P, q, G, h\n",
    "\n",
    "def find_best_k(X_train, y_train, X_test, y_test, loss):\n",
    "    \"\"\"Returns index of maximum gamma that minimizes the mae loss\"\"\"\n",
    "    loss = {}\n",
    "    for k in range(N):\n",
    "        P, q, G, h = generate_params(X_train, y_train, k)\n",
    "        gammas = cvxopt_solve_qp(P,q, G, h)\n",
    "        y_pred = X_test@gammas\n",
    "        loss[k] = mape(y_test,y_pred)\n",
    "    return min(loss, key=loss.get)\n",
    "\n",
    "\n",
    "def cvxopt_solve_qp(P, q, G=None, h=None, A=None, b=None):\n",
    "    P = .5 * (P + P.T)  # make sure P is symmetric\n",
    "    args = [cvxopt.matrix(P), cvxopt.matrix(q)]\n",
    "    if G is not None:\n",
    "        args.extend([cvxopt.matrix(G), cvxopt.matrix(h)])\n",
    "        if A is not None:\n",
    "            args.extend([cvxopt.matrix(A), cvxopt.matrix(b)])\n",
    "    sol = cvxopt.solvers.qp(*args)\n",
    "    if 'optimal' not in sol['status']:\n",
    "        return None\n",
    "    \n",
    "    return np.array(sol['x']).reshape((P.shape[1],))\n",
    "\n",
    "# ----------------------------#\n",
    "#        LOSS FUNCTIONS \n",
    "# ----------------------------#\n",
    "\n",
    "def mape(y_test, y_pred):\n",
    "    return np.mean(np.abs((y_pred-y_test)/y_test))\n",
    "\n",
    "def mspe(y_test, y_pred):\n",
    "    return np.mean(np.square((y_pred-y_test)/y_test))\n",
    "\n",
    "# ----------------------------#\n",
    "#        SMOOTHING\n",
    "# ----------------------------#\n",
    "\n",
    "def simple_exponential_smoothing(series, alpha):\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result\n",
    "\n",
    "def exponential_smoothing(x, rho, K):\n",
    "    const = (1-rho)/(1-rho**(K+1))\n",
    "    new_x = []\n",
    "    \n",
    "    # range of x\n",
    "    r_x = np.arange(K, len(x)-K)\n",
    "\n",
    "    # range of k\n",
    "    r_k = np.arange(0,K)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        if i not in r_x:\n",
    "            new_x.append(x[i])\n",
    "        else:\n",
    "            ls = []\n",
    "            for k in r_k:\n",
    "                ls.append(int(rho**k*x[i-k]))\n",
    "            new_x.append(np.sum(ls))\n",
    "                \n",
    "    return new_x\n",
    "\n",
    "def find_best_alpha():\n",
    "    \"\"\"Returns optimal alpha such that MAPE error is minimized,along with the MAPE index error in question, and its value\"\"\"\n",
    "    X_new = np.zeros(X.shape)\n",
    "    alphas = np.linspace(0,1,100)\n",
    "    mapes = []\n",
    "    pct_80 = int(np.ceil(80*len(X)/100))\n",
    "\n",
    "\n",
    "    for alpha in alphas:\n",
    "        for j in range(X.shape[1]):\n",
    "            X_new[:,j]= exponential_smoothing(X[:,j], alpha)\n",
    "\n",
    "\n",
    "        X_train, X_test = X_new[:pct_80], X_new[pct_80:]\n",
    "        y_train, y_test =y_rec_smoothed[:pct_80], y_rec_smoothed[pct_80:]\n",
    "\n",
    "\n",
    "        index = find_best_k(X_train, y_train, X_test, y_test, 'mape')\n",
    "        P, q, G, h = generate_params(X_train, y_train, index)\n",
    "        gamma = cvxopt_solve_qp(P, q, G, h)\n",
    "        y_pred = X_test@gamma\n",
    "\n",
    "        mapes.append(mape(y_test, y_pred))\n",
    "\n",
    "    return alphas[np.argmin(mapes)],np.argmin(mapes), min(mapes)\n",
    "\n",
    "\n",
    "# ----------------------------#\n",
    "#        GENERATE PREDICTIONS \n",
    "# ----------------------------#\n",
    "\n",
    "index = find_best_k(X_train, y_train, X_test, y_test, 'mape')\n",
    "P, q, G, h = generate_params(X_train, y_train, index)\n",
    "gamma = cvxopt_solve_qp(P, q, G, h)\n",
    "y_pred = X_test@gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'gammas': gamma}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.plot(kind='bar',figsize=(10,8))\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('Mean Absolute percentage error:', mape(y_test, y_pred))  \n",
    "print('Mean Square percentage error:', mspe(y_test, y_pred))  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run same but delete columns 6: and then delete columns 4: (see diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_4 = X[:,:4]\n",
    "X_6 = X[:,:6]\n",
    "N = X_4.shape[1]\n",
    "pct_90 = int(np.ceil(90*len(X_4)/100))\n",
    "pct_80 = int(np.ceil(80*len(X_4)/100))\n",
    "pct_70 = int(np.ceil(70*len(X_4)/100))\n",
    "\n",
    "X_train, X_test = X_4[:pct_80], X_4[pct_80:]\n",
    "y_train, y_test =y_rec_smoothed[:pct_80], y_rec_smoothed[pct_80:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = find_best_k(X_train, y_train, X_test, y_test, 'mape')\n",
    "P, q, G, h = generate_params(X_train, y_train, index)\n",
    "gamma = cvxopt_solve_qp(P, q, G, h)\n",
    "y_pred = X_test@gamma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar',figsize=(10,8))\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_4\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('Mean Absolute percentage error:', mape(y_test, y_pred))  \n",
    "print('Mean Square percentage error:', mspe(y_test, y_pred))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four independent splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the model with **4 gammas** to perform cross validation in order to find the best parameters. As we have around 70 data points, and need at least twice the number of gammas as number of training points, we start with 10 data points. We estimate 10 data points for training, and three for validation. As we have $\\frac{69}{13}= 5.3$, we will do five folds. Each fold has 14 points, except for last fold with 13. We validate on two last data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we split every 13 data points, this is what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_X = np.array_split(X_4, 4, axis=0)\n",
    "splits_y = np.array_split(y_rec_smoothed, 4, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(splits_X, splits_y,lambda_=1.0):  \n",
    "    y_vals = []\n",
    "    y_preds = []\n",
    "    mapes = []\n",
    "    maes = []\n",
    "\n",
    "    for X, y in zip(splits_X, splits_y):\n",
    "        pct_90 = int(np.floor(90*len(X)/100))\n",
    "\n",
    "        X_train = X[:pct_90]\n",
    "        X_val = X[pct_90:]\n",
    "        y_train = y[:pct_90]\n",
    "        y_val = y[pct_90:]\n",
    "        index = find_best_k(X_train, y_train, X_val, y_val, 'mape')\n",
    "        P, q, G, h = generate_params(X_train, y_train, index,lambda_)\n",
    "        gamma = cvxopt_solve_qp(P, q, G, h)\n",
    "        y_pred = X_val@gamma\n",
    "\n",
    "        y_vals.append(y_val)\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "        mapes.append(mape(y_val, y_pred))\n",
    "        maes.append(metrics.mean_absolute_error(y_val, y_pred))\n",
    "        \n",
    "    y_vals = [item for sublist in y_vals for item in sublist]\n",
    "    y_preds =[item for sublist in y_preds for item in sublist]\n",
    "        \n",
    "    return mapes, maes, y_vals, y_preds\n",
    "\n",
    "mapes, maes, y_vals, y_preds = cross_val(splits_X, splits_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "maes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual': y_vals, 'Predicted': y_preds})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar',figsize=(10,8))\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advancement validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want our train set to be of size 40, and then we shift of 10 data points at each new iteration.\n",
    "# the size of our test set is the rest of the dataset points\n",
    "splits = int(np.floor((X_4.shape[0] - 40)/10))\n",
    "\n",
    "##\n",
    "\n",
    "mapes = []\n",
    "maes = []\n",
    "y_vals = []\n",
    "y_preds = []\n",
    "\n",
    "for i in range(splits):\n",
    "\n",
    "    begin = 10*i\n",
    "    end = 40 + 10*i\n",
    "    \n",
    "    X_tr = X_4[begin:end,:]\n",
    "    y_tr = y_rec_smoothed[begin:end]\n",
    "    \n",
    "    X_te = X_4[end:,:]\n",
    "    y_te = y_rec_smoothed[end:]\n",
    "    \n",
    "    # Run the model\n",
    "    \n",
    "    index = find_best_k(X_tr, y_tr, X_te, y_te, 'mape')\n",
    "    P, q, G, h = generate_params(X_tr, y_tr, index,10e-5)\n",
    "    gamma = cvxopt_solve_qp(P, q, G, h)\n",
    "    y_pred = X_te@gamma\n",
    "\n",
    "    y_vals.append(y_te)\n",
    "    y_preds.append(y_pred)\n",
    "\n",
    "    mapes.append(mape(y_te, y_pred))\n",
    "    maes.append(metrics.mean_absolute_error(y_te, y_pred))\n",
    "        \n",
    "y_vals = [item for sublist in y_vals for item in sublist]\n",
    "y_preds =[item for sublist in y_preds for item in sublist]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('for each split we have the following MAPE losses: {}, \\nResulting in a mean MAPE of {}'.format(mapes, np.mean(mapes)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find best hyperparameter $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the function we want to minimize\n",
    "# we want to minimize the mean loss function MAE from our cross validation run\n",
    "def f(lambda_):\n",
    "    mapes, maes, y_vals, y_preds = cross_val(splits_X, splits_y, lambda_)\n",
    "    return np.mean(maes)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "minimize(f,1.0,method='SLSQP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "space  = [Real(10**-5, 10**0, name='learning_rate')]\n",
    "\n",
    "res = gp_minimize(f,space)\n",
    "lambda_ = res['x'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_per_lambda():\n",
    "    lambdas = [-10,-1,0, 10e-5, 10e-4, 10e-3, 10e-2, 10e-1, 1, 10]\n",
    "    mapes = []\n",
    "    for l in lambdas:\n",
    "        X_train = X_4[:pct_80]\n",
    "        X_test = X_4[pct_80:]\n",
    "        y_train = y_recovered[:pct_80]\n",
    "        y_test = y_recovered[pct_80:]\n",
    "        #print(X_test@gamma)\n",
    "        #print(y_test)\n",
    "        index = find_best_k(X_train, y_train, X_test, y_test, 'mape')\n",
    "        P, q, G, h = generate_params(X_train, y_train, index,l)\n",
    "        gamma = cvxopt_solve_qp(P, q, G, h)\n",
    "        y_pred = X_test@gamma\n",
    "        mapes.append(format(100*mape(y_test, y_pred),'.20'))\n",
    "    print(mapes)\n",
    "    print(len(mapes) == len(np.unique(mapes)))\n",
    "    lambdas1 = ['-10','-1','0','10e-5', '10e-4', '10e-3', '10e-2', '10e-1', '1', '10']\n",
    "    plt.plot(lambdas1, mapes, 'b')\n",
    "        #plt.xlabel('Day')\n",
    "        #plt.ylabel('Number of Daily Recovered')\n",
    "        #plt.legend(['Predicted value','True value'])\n",
    "        #plt.title('Baseline Prediction model for k=' + str(k))\n",
    "        #plt.axvline(x=pct_80-1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_per_lambda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gammas_per_lambda():\n",
    "    lambdas = [-10, -1, 0, 10e-5, 10e-4, 10e-3, 10e-2, 10e-1, 1, 10]\n",
    "    gammas = []\n",
    "    for l in lambdas:\n",
    "        X_train = X_4[:pct_80]\n",
    "        X_test = X_4[pct_80:]\n",
    "        y_train = y_recovered[:pct_80]\n",
    "        y_test = y_recovered[pct_80:]\n",
    "        #print(X_test@gamma)\n",
    "        #print(y_test)\n",
    "        index = find_best_k(X_train, y_train, X_test, y_test, 'mape')\n",
    "        P, q, G, h = generate_params(X_train, y_train, index,l)\n",
    "        gamma = cvxopt_solve_qp(P, q, G, h)\n",
    "        y_pred = X_test@gamma\n",
    "        gammas.append(format(np.mean(gamma), '.20f'))\n",
    "    print(gammas)\n",
    "    lambdas1 = ['-10','-1','0','10e-5', '10e-4', '10e-3', '10e-2', '10e-1', '1', '10']\n",
    "\n",
    "    plt.plot(lambdas1, gammas, 'b')\n",
    "        #plt.xlabel('Day')\n",
    "        #plt.ylabel('Number of Daily Recovered')\n",
    "        #plt.legend(['Predicted value','True value'])\n",
    "        #plt.title('Baseline Prediction model for k=' + str(k))\n",
    "        #plt.axvline(x=pct_80-1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gammas_per_lambda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
